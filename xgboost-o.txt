install.packages("xgboost")
install.packages("readr")
install.packages("stringr")
install.packages("caret")
install.packages("e1071")
install.packages("data.table")
install.packages("mlr")
install.packages("ParamHelpers")
install.packages("rts")
install.packages("dplyr")
install.packages("ncdf4")
install.packages("lubridate")
install.packages("reshape2")
install.packages("rsample")

#load libraries
 library(data.table)
 library(mlr)
 library(xgboost)
 library(readr)
 library("rts")
 library("dplyr")
 library("ncdf4")
 library("lubridate")
 library("reshape2")
 library("rsample")




# load data

path <- "C:\\Education\\Education\\Upcoming papers\\Geothermale\\Water journal"
setwd("C:\\Education\\Education\\Upcoming papers\\Geothermale\\Water journal")

df_train = read_csv("OPPC.csv")
df_test = read_csv("OPPCT.csv")

#set variable names
predictors <- c("p",
            "te",
            "v",
            "nao",
            "best",
            "soi",
            "ido",
            "qbo")
            
response<-"o"

training<-df_train [,c(predictors,response)]

test<-df_test [,c(predictors,response)]

xgb.model.inputs<-colnames(dplyr::select(training,-c(all_of(response))))

training<-training[complete.cases(training),] #select only complete cases

test<-test[complete.cases(test),] #select only complete cases

library(caret)

do.oo <- caret::preProcess(subset(training,select=-c(o)), # last column is the outcome 
method = c("center", "scale"))

do.oo #this will show which columns were centered and scaled
#%%%%%saveRDS(do.oo,file="do.RDS")

training.tr <- predict(do.oo, newdata = subset(training,select=-c(o)))
training.target<-subset(training,select=c(o))

test.tr <- predict(do.oo, newdata = subset(test,select=-c(o)))
test.target<-subset(test,select=c(o))

full.tr <- rbind(training.tr,test.tr)
full.target<-rbind(training.target,test.target)

training.target<-as.numeric(training.target$o)
test.target<-as.numeric(test.target$o)
full.target<-as.numeric(full.target$o)


### directly with xgboost package
dtrain <- xgboost::xgb.DMatrix(data = as.matrix(training.tr), label = training.target)
dtest <- xgboost::xgb.DMatrix(data = as.matrix(test.tr), label = test.target)
dfull<-xgboost::xgb.DMatrix(data = as.matrix(full.tr), label = full.target)


cv.ctrl <- caret::trainControl(method = "cv",number = 4,
                               verboseIter=TRUE,
                               allowParallel=T)

#run repeated tune grids to optimize hyperparamters, then select best combination for full training
grid.number<-1
#hyperparameter fitting - full grid is much to large to process 
xgbGrid <- expand.grid(max_depth = c(2,4,7,8,9,12,16),
                       eta = c(0.05,0.1,0.3),
                       rate_drop = c(0,0.01,0.02),
                       skip_drop = c(0,0.01,0.02),
                       min_child_weight = c(0,2,4,8,12,14,16,20),
                       subsample = c(0.7,0.8,0.85,0.9,0.95,1),
                       colsample_bytree = c(0.7,0.8,0.85,0.9,0.95,1),
                       gamma = c(0,2,5,10,20,100,500,1000),
                       nrounds = c(250,300,400)
)

set.seed(1234)
xgb_tune <-caret::train(training.tr,training.target,
                        method="xgbDART",
                        trControl=cv.ctrl,
                        tuneGrid=xgbGrid[sample(1:nrow(xgbGrid), 200),], #limit number of samples to try from grid
                        verbose=T,
                        metric="RMSE"
)


xgb.tune.df<-xgb_tune$results
xgb.tune.df$grid.number<-grid.number
#use the following line after the first grid only
xgb_grid30<-xgb.tune.df

#run the following line to append results from later rounds of grid searching
# xgb_grid30<-rbind(xgb_grid30,xgb.tune.df)

#%%%write.csv(xgb_grid30, file = "do.xgb_grid30_srv.csv")

tuneround="1-center,scale - dxs cut -30 50 - test 0.9-watch 0.9 split" #tune round is the number and can be used to export comments 
param <- list(booster = "dart",
              verbosity = 1,
              max_depth = 9, 
              eta = 0.1,
              rate_drop = 0.01,
              skip_drop = 0,
              min_child_weight = 12,
              subsample = 0.85,
              colsample_bytree = 0.9,
              gamma = 5,
              objective = "reg:linear", 
              eval_metric = "rmse")

nrounds = 1500
set.seed(1234)
bst <- xgb.train(param, dtrain, nrounds = nrounds)


#save model in 2 formats as a precaution
#%%%%saveRDS(bst,file="do.model1RDS")

do.featureIDs<-as.character(bst$feature_names)
#%%%%saveRDS(dP.featureIDs,file="dP.featureIDs")
#%%%%xgb.save(bst,"dP.model1XGB")

set.seed(1234)
pred <- predict(bst, dtest, nrounds = nrounds)
pred.test<-as.data.frame(cbind(pred, test.target))
print (pred.test)
param$test.rmse<-RMSE(pred.test$pred,pred.test$test.target)
param$test.rmse

set.seed(1234)
pred <- predict(bst, dtrain, nrounds = nrounds)
pred.train<-as.data.frame(cbind(pred, training.target))
print (pred.train)
param$train.rmse<-RMSE(pred.train$pred,pred.train$training.target)
param$train.rmse

set.seed(1234)
pred <- predict(bst, dfull, nrounds = nrounds)
pred.full<-as.data.frame(cbind(pred, full.target))
print (pred.full)
param$full.rmse<-RMSE(pred.full$pred,pred.full$full.target)
param$full.rmse

param$nrounds<-nrounds
param$tuneround<-tuneround
param<-as.data.frame(param)
xgbDart.tuning30<-param

#importance
importance <- xgb.importance(feature_names = names(training.tr), model = bst)
print (importance)

























